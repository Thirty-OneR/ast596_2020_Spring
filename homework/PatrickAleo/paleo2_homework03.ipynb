{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASTR596, FDS: Homework set 3 - Moving from MLE and Frequentism to evaluating the Posterior distribution and Bayesian Statistics\n",
    "\n",
    "\n",
    "\n",
    "## Problem 1\n",
    "\n",
    "Last week you showed that Bayesian and Frequentist approaches are often equivalent for simple problems (after all you implicitly had a prior in the form of a grid). \n",
    "\n",
    "But it is also true that they can diverge greatly. In practice, this divergence makes itself most clear in two different situations:\n",
    "\n",
    "1. The handling of \"nuisance parameters\"\n",
    "2. The subtle (and often overlooked) difference between frequentist confidence intervals and Bayesian credible intervals\n",
    "\n",
    "Here, we focus on the first point: the difference between frequentist and Bayesian treatment of nuisance parameters.\n",
    "\n",
    "\n",
    "## What is a Nuisance Parameter?\n",
    "\n",
    "***A nuisance parameter is any quantity whose value is not relevant to the goal of an analysis, but is nevertheless required to determine some quantity of interest***.\n",
    "\n",
    "For example, last week, in problem 3, we estimated both the mean $C$ and shape $\\sigma_\\text{source}$ for some spherical cow galaxy. Now the shape might be important, but in homework 1, you only needed the brightness of the sources where you plotted up $r$ vs $g-i$ for galaxies (you needed the shape to separate stars from galaxies in the form of MEAN_OBJECT_TYPE, but that's it).\n",
    "\n",
    "As far as you were concerned in homework 1, the shape was a **nuisance** parameter.\n",
    "\n",
    "\n",
    "## A Classic Problem:\n",
    "\n",
    "\n",
    "We'll start with an example of nuisance parameters that, in one form or another, dates all the way back to the posthumous [1763 paper](http://www.stat.ucla.edu/history/essay.pdf) written by Thomas Bayes himself. The particular version of this problem we'll study is borrowed from [Eddy 2004](ftp://selab.janelia.org/pub/publications/Eddy-ATG3/Eddy-ATG3-reprint.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The setting is a rather contrived game in which Alice and Bob bet on the outcome of a process they can't directly observe:\n",
    "\n",
    "> Alice and Bob enter a room. Behind a curtain there is a billiard table, which they cannot see, but their friend Carol can. Carol rolls a ball down the table, and marks where it lands. Once this mark is in place, Carol begins rolling new balls down the table. If the ball lands to the left of the mark, Alice gets a point; if it lands to the right of the mark, Bob gets a point.  We can assume for the sake of example that Carol's rolls are unbiased: that is, the balls have an equal chance of ending up anywhere on the table.  **The first person to reach six points wins the game.**\n",
    "\n",
    "Here ***the location of the mark (determined by the first roll) can be considered a nuisance parameter***.\n",
    "\n",
    "It is unknown, and not of immediate interest, but it clearly must be accounted for when predicting the outcome of subsequent rolls. If the first roll settles far to the right, then subsequent rolls will favor Alice. If it settles far to the left, Bob will be favored instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this setup, here is the question we ask of ourselves:\n",
    "\n",
    "> In a particular game, after eight rolls, Alice has five points and Bob has three points. What is the probability that Bob will go on to win the game?\n",
    "\n",
    "Intuitively, you probably realize that because Alice received five of the eight points, the marker placement likely favors her. And given this, it's more likely that the next roll will go her way as well. \n",
    "\n",
    "And she has three opportunities to get a favorable roll before Bob can win; she seems to have clinched it.  But, **quantitatively**, what is the probability that Bob will squeak-out a win?\n",
    "\n",
    "\n",
    "Someone following a classical frequentist approach might reason as follows:\n",
    "\n",
    "To determine the result, we need an intermediate estimate of where the marker sits. We'll quantify this marker placement as a probability $p$ that any given roll lands in Alice's favor.  Because five balls out of eight fell on Alice's side of the marker, we can quickly show that the maximum likelihood estimate of $p$ is given by:\n",
    "\n",
    "$$\n",
    "\\hat{p} = 5/8\n",
    "$$\n",
    "\n",
    "(This result follows in a straightforward manner from the [binomial likelihood](http://en.wikipedia.org/wiki/Binomial_distribution)). \n",
    "\n",
    "# 1.1 Under the assumptions of maximum likelihood, what is the probability of Bob winning the game - i.e. getting six points, and what are the odds \n",
    "# (i.e. $(1 - p_\\text{Bob wins})/p_\\text{Bob wins}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bob needs to pray to whatever god he believes in and get 3 rolls in a row in his favor. This is represented by\n",
    "> $p_{Bob wins} = (1-p_{Alice wins})^3 = (1-\\hat{p})^3$\n",
    "\n",
    "where the cube comes from having 3 consecutive rolls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bob's chance of winning is... 0.052734375\n",
      "The odds against Bob winning is... 18 to 1\n"
     ]
    }
   ],
   "source": [
    "prob_alice_win = 5/8 #this is p_hat\n",
    "prob_bob_win = (1 - prob_alice_win)**3 \n",
    "odds_agst_bob = (1 - prob_bob_win) / prob_bob_win\n",
    "\n",
    "print(\"Bob's chance of winning is...\", prob_bob_win)\n",
    "print(\"The odds against Bob winning is... {0:.0f} to 1\".format(odds_agst_bob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also approach this problem from a Bayesian standpoint. This is slightly more involved, and requires us to first define some notation.\n",
    "\n",
    "We'll consider the following random variables:\n",
    "\n",
    "- $B$ = Bob Wins\n",
    "- $D$ = observed data, i.e. $D = (n_A, n_B) = (5, 3)$\n",
    "- $p$ = unknown probability that a ball lands on Alice's side during the current game\n",
    "\n",
    "We want to compute $P(B~|~D)$; that is, the probability that Bob wins given our observation that Alice currently has five points to Bob's three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general Bayesian method of treating nuisance parameters is ***marginalization***, or integrating the joint probability over the entire range of the nuisance parameter. In this case, that means that we will first calculate the joint distribution\n",
    "$$\n",
    "P(B,p~|~D)\n",
    "$$\n",
    "and then marginalize over $p$ using the following identity:\n",
    "$$\n",
    "P(B~|~D) \\equiv \\int_{-\\infty}^\\infty P(B,p~|~D) {\\mathrm d}p\n",
    "$$\n",
    "This identity follows from the definition of conditional probability, and the law of total probability: that is, it is a fundamental consequence of probability axioms and will always be true. Even a frequentist would recognize this; they would simply disagree with our interpretation of $P(p)$ as being a measure of uncertainty of our own knowledge.\n",
    "\n",
    "To compute this result, we will manipulate the above expression for $P(B~|~D)$ until we can express it in terms of other quantities that we can compute.\n",
    "\n",
    "We'll start by applying the following definition of [conditional probability](http://en.wikipedia.org/wiki/Conditional_probability#Definition) to expand the term $P(B,p~|~D)$:\n",
    "\n",
    "$$\n",
    "P(B~|~D) = \\int P(B~|~p, D) P(p~|~D) dp\n",
    "$$\n",
    "\n",
    "Next we use [Bayes' rule](http://en.wikipedia.org/wiki/Bayes%27_theorem) to rewrite $P(p~|~D)$:\n",
    "\n",
    "$$\n",
    "P(B~|~D) = \\int P(B~|~p, D) \\frac{P(D~|~p)P(p)}{P(D)} dp\n",
    "$$\n",
    "\n",
    "Finally, using the same probability identity we started with, we can expand $P(D)$ in the denominator to find:\n",
    "\n",
    "$$\n",
    "P(B~|~D) = \\frac{\\int P(B~|~p,D) P(D~|~p) P(p) dp}{\\int P(D~|~p)P(p) dp}\n",
    "$$\n",
    "\n",
    "Now the desired probability is expressed in terms of three quantities that we can compute. Let's look at each of these in turn:\n",
    "<small>\n",
    "- $P(B~|~p,D)$: This term is exactly the frequentist likelihood we used above. In words: given a marker placement $p$ and the fact that Alice has won 5 times and Bob 3 times, what is the probability that Bob will go on to six wins?  \n",
    "    \n",
    "    \n",
    "- $P(D~|~p)$: this is another easy-to-compute term. In words: given a probability $p$, what is the likelihood of exactly 5 positive outcomes out of eight trials? The answer comes from the well-known [Binomial distribution](http://en.wikipedia.org/wiki/Binomial_distribution)\n",
    "    \n",
    "    \n",
    "- $P(p)$: this is our prior on the probability $p$. By the problem definition, we can assume that $p$ is evenly drawn between 0 and 1.  That is, $P(p) \\propto 1$, and the integrals range from 0 to 1.</small>\n",
    "\n",
    "# 1.2 Evaluate that integral and get an expression in terms of $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paleo's answer:\n",
    "\n",
    "Currently we have our desired probability as\n",
    "$$\n",
    "P(B~|~D) = \\frac{\\int P(B~|~p,D) P(D~|~p) P(p) dp}{\\int P(D~|~p)P(p) dp}.\n",
    "$$\n",
    "\n",
    "Thankfully, we already calculated $P(B~|~p,D)$ in part 1.1. This is simply $P(B~|~p,D)=(1-p)^3$. :)\n",
    "\n",
    "Also, we know the prior to be $P(p) \\propto 1$ (for p evaluated between 0 and 1), and so this makes things easy as it does not affect the integrand. It is not dependent on $p$.\n",
    "\n",
    "Lastly, we need to find $P(D~|~p)$. Well, this is simply from the Binomial distribution's pmf. The probability of getting exactly $k$ successes in $n$ independent Bernoulli trials is given by the pmf is for our situation\n",
    "$$\n",
    "{n \\choose k} p^k (1-p)^{(n-k)} = {8 \\choose 5} p^5 (1-p)^{(8-5)} ={8 \\choose 5} p^5 (1-p)^{3}.\n",
    "$$\n",
    "Because the expression $P(D~|~p)$ is in the numerator and denominator and doesn't depend on $p$, the ${8 \\choose 5}$ part cancels out. Thus, our expression is found via substitution:\n",
    "$$\n",
    "P(B~|~D) = \\frac{\\int P(B~|~p,D) P(D~|~p) P(p) dp}{\\int P(D~|~p)P(p) dp} = \\frac{\\int_0^1 p^5 (1-p)^6 dp}{\\int_0^1 p^5 (1-p)^3 dp}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integrals you find might look a bit difficult, but they are just special cases of the [Beta Function](http://en.wikipedia.org/wiki/Beta_function):\n",
    "$$\n",
    "\\beta(n, m) = \\int_0^1 (1 - p)^{n - 1} p^{m - 1}\n",
    "$$\n",
    "\n",
    "scipy has an implementation of this in `scipy.special`.\n",
    "\n",
    "# 1.3 Evaluate your expression numerically and compute the probability and the odds that Bob wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of Bob winning is given by... P(B|D) = 0.091\n",
      "The odds against Bob winning is... 10 to 1\n"
     ]
    }
   ],
   "source": [
    "# paleo's answer here\n",
    "\n",
    "from scipy.special import beta\n",
    "bayes_prob_bob_win = (beta(6, 7) / beta(6, 4)) # +1 to balance out -1 in beta function for scipy\n",
    "odds_against_B = (1 - bayes_prob_bob_win) / bayes_prob_bob_win\n",
    "                                                      \n",
    "\n",
    "print(\"The probability of Bob winning is given by... P(B|D) = {0:.3f}\".format(bayes_prob_bob_win))\n",
    "print(\"The odds against Bob winning is... {0:.0f} to 1\".format(odds_against_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, this is better than 18 to 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Finally, lets check the result using an approach called Monte Carlo, where we simulate a bunch of games, and simply count the fraction of relevant games that Bob goes on to win. The current problem is especially simple because so many of the random variables involved are uniformly distributed. \n",
    "\n",
    "* Simulate 100,000 random p between 0 and 1 - this will be a 1D array\n",
    "* given each p, to win the game needs *at most* 11 rolls - simulate 11 rolls for each p - this will be a 2D array\n",
    "* count the cumultative wins for Alice and Bob at each roll - this is a 2D array\n",
    "* determine which games has Bob with three points by the end of game 8 - this is a 1D array\n",
    "* considering only these games, find the number of games which Bob won (i.e. has six points at the end of game 11)\n",
    "* compute the total probability that Bob won, and the odds that Bob won.\n",
    "\n",
    "\n",
    "## You don't need anything more than `numpy` to do this - in particular `numpy.random` and `numpy.cumsum` to do this.\n",
    "No for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paleo's answer\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(10) #random seed is 10 b/c it's my favorite number\n",
    "p = np.random.uniform(0,1,100000) #100k games\n",
    "\n",
    "#winner needs at most 11 rolls for each p - 2D array\n",
    "roll_tot = np.random.random((11, 100000))\n",
    "#cumulative wins for Bob, Alice - 2D array\n",
    "bob_win_N = np.cumsum(roll_tot >= p, 0) #if roll count hits 11, Bob wins (cause he gets next 3 from 8)\n",
    "alice_win_N = np.cumsum(roll_tot < p, 0) #if roll count < 11, Alice wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# games Bob won is... 972 out of 11073 playable games from 100000\n",
      "prob bob win(!) is... 0.088\n",
      "The odds against Bob winning is... 10 to 1\n"
     ]
    }
   ],
   "source": [
    "# get number of games to get us in this situation\n",
    "playable = bob_win_N[7] == 3 #Bob gets next 3 rolls after the 8th (remember python index starts @ 0)\n",
    "playable_tot = np.sum(playable) #total playable games out of 100k\n",
    "\n",
    "# out of playable games, how many did bob win? alice win?\n",
    "bob_win_N = bob_win_N[:, playable]\n",
    "#print(bob_win_N)\n",
    "alice_win_N = alice_win_N[:, playable]\n",
    "bob_won = np.sum(np.sum(bob_win_N[10] == 6)) # bob gets 6 points to win out of 11 rolls\n",
    "print(\"# games Bob won is... {0}\".format(bob_won), \"out of\", playable_tot, \"playable games from 100000\")\n",
    "\n",
    "prob_bob_win = bob_won * (1 / np.sum(playable)) # compute total probability bob win\n",
    "odds_against_bob = (1 - prob_bob_win) / prob_bob_win\n",
    "\n",
    "print(\"prob bob win(!) is... {0:.3f}\".format(prob_bob_win))\n",
    "print(\"The odds against Bob winning is... {0:.0f} to 1\".format(odds_against_bob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Why do the two results disagree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# paleo's answer\n",
    "\n",
    "In short, the Bayesian result agreed with the brute force Monte Carlo result at 10 to 1 odds against Bob winning, but differed from the Frequentist approach at 18 to 1 odds. This seems to be because of the nuisance parameter, and how the frequentist approach deals with it (or rather, **doesn't**) in this problem. Earlier, we defined $p$ as the \"unknown probability that a ball lands on Alice's side during the current game\", and the nuisance parameter to be \"the location of the mark (determined by the first roll)\". In this setup, $p$ and the nuisance parameter are linked in that the value of $p$ depends very much on where the location of the initial ball is, and in the frequentist approach there's no way to take this info into account when calculating our probability- we just say \"hey, this is an independent event like rolling die, where previous rolls don't depend on anything before\", whereas in the Bayesian approach we do take this into account and marginalize over $p$.\n",
    "\n",
    "Basically, there is further information with which we can refine our probability estimate but we don't do so in naive frequentist approach-- we ignore the nuisance parameter in general and assume $p$'s independence. However, in reality there is some tie in between the two and we need to do some algebraic wizardry to account for it, which is what the Bayesian (and brute MC) approach does. Maybe there's a way to mitigate the problem's dependence on $p$ to get a more similar answer between the different methods?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:fds] *",
   "language": "python",
   "name": "conda-env-fds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
