{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASTR596, FDS: Homework set 3 - Moving from MLE and Frequentism to evaluating the Posterior distribution and Bayesian Statistics\n",
    "\n",
    "\n",
    "\n",
    "## Problem 1\n",
    "\n",
    "Last week you showed that Bayesian and Frequentist approaches are often equivalent for simple problems (after all you implicitly had a prior in the form of a grid). \n",
    "\n",
    "But it is also true that they can diverge greatly. In practice, this divergence makes itself most clear in two different situations:\n",
    "\n",
    "1. The handling of \"nuisance parameters\"\n",
    "2. The subtle (and often overlooked) difference between frequentist confidence intervals and Bayesian credible intervals\n",
    "\n",
    "Here, we focus on the first point: the difference between frequentist and Bayesian treatment of nuisance parameters.\n",
    "\n",
    "\n",
    "## What is a Nuisance Parameter?\n",
    "\n",
    "***A nuisance parameter is any quantity whose value is not relevant to the goal of an analysis, but is nevertheless required to determine some quantity of interest***.\n",
    "\n",
    "For example, last week, in problem 3, we estimated both the mean $C$ and shape $\\sigma_\\text{source}$ for some spherical cow galaxy. Now the shape might be important, but in homework 1, you only needed the brightness of the sources where you plotted up $r$ vs $g-i$ for galaxies (you needed the shape to separate stars from galaxies in the form of MEAN_OBJECT_TYPE, but that's it).\n",
    "\n",
    "As far as you were concerned in homework 1, the shape was a **nuisance** parameter.\n",
    "\n",
    "\n",
    "## A Classic Problem:\n",
    "\n",
    "\n",
    "We'll start with an example of nuisance parameters that, in one form or another, dates all the way back to the posthumous [1763 paper](http://www.stat.ucla.edu/history/essay.pdf) written by Thomas Bayes himself. The particular version of this problem we'll study is borrowed from [Eddy 2004](ftp://selab.janelia.org/pub/publications/Eddy-ATG3/Eddy-ATG3-reprint.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The setting is a rather contrived game in which Alice and Bob bet on the outcome of a process they can't directly observe:\n",
    "\n",
    "> Alice and Bob enter a room. Behind a curtain there is a billiard table, which they cannot see, but their friend Carol can. Carol rolls a ball down the table, and marks where it lands. Once this mark is in place, Carol begins rolling new balls down the table. If the ball lands to the left of the mark, Alice gets a point; if it lands to the right of the mark, Bob gets a point.  We can assume for the sake of example that Carol's rolls are unbiased: that is, the balls have an equal chance of ending up anywhere on the table.  **The first person to reach six points wins the game.**\n",
    "\n",
    "Here ***the location of the mark (determined by the first roll) can be considered a nuisance parameter***.\n",
    "\n",
    "It is unknown, and not of immediate interest, but it clearly must be accounted for when predicting the outcome of subsequent rolls. If the first roll settles far to the right, then subsequent rolls will favor Alice. If it settles far to the left, Bob will be favored instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this setup, here is the question we ask of ourselves:\n",
    "\n",
    "> In a particular game, after eight rolls, Alice has five points and Bob has three points. What is the probability that Bob will go on to win the game?\n",
    "\n",
    "Intuitively, you probably realize that because Alice received five of the eight points, the marker placement likely favors her. And given this, it's more likely that the next roll will go her way as well. \n",
    "\n",
    "And she has three opportunities to get a favorable roll before Bob can win; she seems to have clinched it.  But, **quantitatively**, what is the probability that Bob will squeak-out a win?\n",
    "\n",
    "\n",
    "Someone following a classical frequentist approach might reason as follows:\n",
    "\n",
    "To determine the result, we need an intermediate estimate of where the marker sits. We'll quantify this marker placement as a probability $p$ that any given roll lands in Alice's favor.  Because five balls out of eight fell on Alice's side of the marker, we can quickly show that the maximum likelihood estimate of $p$ is given by:\n",
    "\n",
    "$$\n",
    "\\hat{p} = 5/8\n",
    "$$\n",
    "\n",
    "(This result follows in a straightforward manner from the [binomial likelihood](http://en.wikipedia.org/wiki/Binomial_distribution)). \n",
    "\n",
    "# 1.1 Under the assumptions of maximum likelihood, what is the probability of Bob winning the game - i.e. getting six points, and what are the odds \n",
    "# (i.e. $(1 - p_\\text{Bob wins})/p_\\text{Bob wins}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of Bob winning is 5.2734375 %\n",
      "The odds of Bob winning are 17.962962962962962\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "## your answer here\n",
    "#In general, if the random variable X follows the binomial distribution with parameters n ∈ ℕ and p ∈ [0,1]\n",
    "#The probability of getting exactly k successes in n independent Bernoulli trials\n",
    "#is given by the probability mass function\n",
    "\n",
    "\n",
    "def probability_function(p,n,k):\n",
    "    x_k = math.factorial(n)/ (math.factorial(k)* math.factorial(n-k))\n",
    "    P = x_k * (p**k) * ((1-p)**(n-k))\n",
    "    return P\n",
    "    \n",
    "#Bob needs to have k = 3 successes in n = 3 trials with probability of him getting a point being p = 3/8 to win.\n",
    "\n",
    "P_bob = probability_function(p=3/8, n = 3, k = 3) * 100 ##probability percent\n",
    "odds_bob = (1-(P_bob/100)) / (P_bob/100)\n",
    "\n",
    "print(\"Probability of Bob winning is\",P_bob, \"%\")\n",
    "print(\"The odds of Bob winning are\",odds_bob)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also approach this problem from a Bayesian standpoint. This is slightly more involved, and requires us to first define some notation.\n",
    "\n",
    "We'll consider the following random variables:\n",
    "\n",
    "- $B$ = Bob Wins\n",
    "- $D$ = observed data, i.e. $D = (n_A, n_B) = (5, 3)$\n",
    "- $p$ = unknown probability that a ball lands on Alice's side during the current game\n",
    "\n",
    "We want to compute $P(B~|~D)$; that is, the probability that Bob wins given our observation that Alice currently has five points to Bob's three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general Bayesian method of treating nuisance parameters is ***marginalization***, or integrating the joint probability over the entire range of the nuisance parameter. In this case, that means that we will first calculate the joint distribution\n",
    "$$\n",
    "P(B,p~|~D)\n",
    "$$\n",
    "and then marginalize over $p$ using the following identity:\n",
    "$$\n",
    "P(B~|~D) \\equiv \\int_{-\\infty}^\\infty P(B,p~|~D) {\\mathrm d}p\n",
    "$$\n",
    "This identity follows from the definition of conditional probability, and the law of total probability: that is, it is a fundamental consequence of probability axioms and will always be true. Even a frequentist would recognize this; they would simply disagree with our interpretation of $P(p)$ as being a measure of uncertainty of our own knowledge.\n",
    "\n",
    "To compute this result, we will manipulate the above expression for $P(B~|~D)$ until we can express it in terms of other quantities that we can compute.\n",
    "\n",
    "We'll start by applying the following definition of [conditional probability](http://en.wikipedia.org/wiki/Conditional_probability#Definition) to expand the term $P(B,p~|~D)$:\n",
    "\n",
    "$$\n",
    "P(B~|~D) = \\int P(B~|~p, D) P(p~|~D) dp\n",
    "$$\n",
    "\n",
    "Next we use [Bayes' rule](http://en.wikipedia.org/wiki/Bayes%27_theorem) to rewrite $P(p~|~D)$:\n",
    "\n",
    "$$\n",
    "P(B~|~D) = \\int P(B~|~p, D) \\frac{P(D~|~p)P(p)}{P(D)} dp\n",
    "$$\n",
    "\n",
    "Finally, using the same probability identity we started with, we can expand $P(D)$ in the denominator to find:\n",
    "\n",
    "$$\n",
    "P(B~|~D) = \\frac{\\int P(B~|~p,D) P(D~|~p) P(p) dp}{\\int P(D~|~p)P(p) dp}\n",
    "$$\n",
    "\n",
    "Now the desired probability is expressed in terms of three quantities that we can compute. Let's look at each of these in turn:\n",
    "<small>\n",
    "- $P(B~|~p,D)$: This term is exactly the frequentist likelihood we used above. In words: given a marker placement $p$ and the fact that Alice has won 5 times and Bob 3 times, what is the probability that Bob will go on to six wins?  \n",
    "    \n",
    "    \n",
    "- $P(D~|~p)$: this is another easy-to-compute term. In words: given a probability $p$, what is the likelihood of exactly 5 positive outcomes out of eight trials? The answer comes from the well-known [Binomial distribution](http://en.wikipedia.org/wiki/Binomial_distribution)\n",
    "    \n",
    "    \n",
    "- $P(p)$: this is our prior on the probability $p$. By the problem definition, we can assume that $p$ is evenly drawn between 0 and 1.  That is, $P(p) \\propto 1$, and the integrals range from 0 to 1.</small>\n",
    "\n",
    "# 1.2 Evaluate that integral and get an expression in terms of $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your answer here\n",
    "\n",
    ">$ P(p) = 1 $\n",
    "\n",
    ">$ P(B\\mid p,D) = (1-p)^{n-k} $\n",
    "\n",
    ">$ P(B\\mid p,D) = \\binom{n}{k} \\cot p^{k} \\cdot (1-p)^{n-k} $\n",
    "\n",
    "where\n",
    ">$ \\binom{n}{k} = \\frac{n!}{k! \\cdot (n-k)!} $\n",
    "\n",
    ">$P(B\\mid D) = \\frac{\\int_{0}^{1} P(B\\mid p,D)\\cdot P(D\\mid p)\\cdot P(p) dp}{\\int_{0}^{1}P(D\\mid p)\\cdot P(p) dp}$\n",
    "\n",
    ">$ P(B\\mid D) =\\frac{\\int_{0}^{1}(1-p)^{n-k} \\cdot \\binom{n}{k} \\cdot p^{k} \\cdot(1-p)^{n-k} \\cdot (1)dp}{\\int_{0}^{1}\\binom{n}{k} \\cdot p^{k} (1-p)^{n-k} \\cdot (1)dp}$\n",
    "\n",
    ">$P(B\\mid D) =\\frac{\\int_{0}^{1}\\binom{n}{k} \\cdot p^{k} \\cdot (1-p)^{2(n-k)}  dp}{\\int_{0}^{1}\\binom{n}{k} \\cdot p^{k} \\cdot (1-p)^{n-k} dp}$\n",
    "\n",
    "$\\binom{n}{k}$ gets canceled \n",
    "\n",
    ">$ P(B\\mid D) =\\frac{\\int_{0}^{1} p^{k} \\cdot (1-p)^{2(n-k)}  dp}{\\int_{0}^{1} p^{k} \\cdot(1-p)^{n-k} dp}$\n",
    "\n",
    "k = 5, n = 8\n",
    "\n",
    ">$ P(B\\mid D) =\\frac{\\int_{0}^{1} p^{5} \\cdot (1-p)^{2(8-5)}  dp}{\\int_{0}^{1} p^{5} \\cdot(1-p)^{8-5} dp}$\n",
    "\n",
    ">$ P(B\\mid D) =\\frac{\\int_{0}^{1} p^{5} \\cdot (1-p)^{6}  dp}{\\int_{0}^{1} p^{5} \\cdot(1-p)^{3} dp}$\n",
    "\n",
    "this can be rewritten to match the beta function:\n",
    "\n",
    ">$P(B\\mid D) =\\frac{\\int_{0}^{1} p^{6-1} \\cdot (1-p)^{7-1}  dp}{\\int_{0}^{1} p^{6-1} \\cdot(1-p)^{4-1} dp}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integrals you find might look a bit difficult, but they are just special cases of the [Beta Function](http://en.wikipedia.org/wiki/Beta_function):\n",
    "$$\n",
    "\\beta(n, m) = \\int_0^1 (1 - p)^{n - 1} p^{m - 1}\n",
    "$$\n",
    "\n",
    "scipy has an implementation of this in `scipy.special`.\n",
    "\n",
    "# 1.3 Evaluate your expression numerically and compute the probability and the odds that Bob wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the probability that Bob wins given our observations is  9.090909090909092 %\n",
      "The odds of Bob winning are 10.0\n"
     ]
    }
   ],
   "source": [
    "# your answer here\n",
    "from scipy.special import beta as b\n",
    "\n",
    "intergral = b(6,7) / b(6,4)\n",
    "print(\"the probability that Bob wins given our observations is \",intergral*100,\"%\")\n",
    "odds = (1- intergral)/ intergral\n",
    "print(\"The odds of Bob winning are\",odds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Finally, lets check the result using an approach called Monte Carlo, where we simulate a bunch of games, and simply count the fraction of relevant games that Bob goes on to win. The current problem is especially simple because so many of the random variables involved are uniformly distributed. \n",
    "\n",
    "* Simulate 100,000 random p between 0 and 1 - this will be a 1D array\n",
    "* given each p, to win the game needs *at most* 11 rolls - simulate 11 rolls for each p - this will be a 2D array\n",
    "* count the cumultative wins for Alice and Bob at each roll - this is a 2D array\n",
    "* determine which games has Bob with three points by the end of game 8 - this is a 1D array\n",
    "* considering only these games, find the number of games which Bob won (i.e. has six points at the end of game 11)\n",
    "* compute the total probability that Bob won, and the odds that Bob won.\n",
    "\n",
    "\n",
    "## You don't need anything more than `numpy` to do this - in particular `numpy.random` and `numpy.cumsum` to do this.\n",
    "No for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of Bob winning is 1.024 %\n",
      "The odds of Bob winning are 96.65624999999999\n"
     ]
    }
   ],
   "source": [
    "# your answer her\n",
    "\n",
    "# Simulate 100,000 random p between 0 and 1 - this will be a 1D array\n",
    "P_bob = np.random.random(100000)\n",
    "#print(events[1:10])\n",
    "\n",
    "# given each p, to win the game needs at most 11 rolls(row) - simulate 11 rolls for each p(column) - this will be a 2D array\n",
    "simulate = np.random.binomial(1, P_bob, size= (11,len(events)))\n",
    "\n",
    "# count the cumultative wins for Alice and Bob at each roll - this is a 2D array\n",
    "sum_ = np.cumsum(simulate, axis=0)\n",
    "\n",
    "# determine which games has Bob with three points by the end of game 8 - this is a 1D array\n",
    "sum_8th_game = sum_[7][:]\n",
    "games_bob_3 = np.where(sum_8th_game == 3)\n",
    "\n",
    "# considering only these games, find the number of games which Bob won (i.e. has six points at the end of game 11)\n",
    "relevent_11th_game = sum_[-1][sum_8th_game ==3]\n",
    "bob_won = np.where(relevent_11th_game == 6)\n",
    "\n",
    "# compute the total probability that Bob won, and the odds that Bob won\n",
    "prob = (len(bob_won[0])/len(P_bob))\n",
    "odds = (1-prob)/prob\n",
    "\n",
    "print(\"Probability of Bob winning is\",prob*100, \"%\")\n",
    "print(\"The odds of Bob winning are\",odds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Why do the two results disagree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your answer here\n",
    "\n",
    "As explained in class, the between the frequentist method vs. Bayesian method is how each method obtains its probability.  The frequentist method considered a probability that is constant for the entire situation based on the observations we already made. the Bayesian method considers the probability as a range of all possibilities while using the observations as initial condition. This is why the probability of bob winning obtained from each method do not agree with eachother.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:fds] *",
   "language": "python",
   "name": "conda-env-fds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
