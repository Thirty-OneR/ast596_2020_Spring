{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASTR596, FDS: Homework set 3 - Moving from MLE and Frequentism to evaluating the Posterior distribution and Bayesian Statistics\n",
    "\n",
    "\n",
    "\n",
    "## Problem 1\n",
    "\n",
    "Last week you showed that Bayesian and Frequentist approaches are often equivalent for simple problems (after all you implicitly had a prior in the form of a grid). \n",
    "\n",
    "But it is also true that they can diverge greatly. In practice, this divergence makes itself most clear in two different situations:\n",
    "\n",
    "1. The handling of \"nuisance parameters\"\n",
    "2. The subtle (and often overlooked) difference between frequentist confidence intervals and Bayesian credible intervals\n",
    "\n",
    "Here, we focus on the first point: the difference between frequentist and Bayesian treatment of nuisance parameters.\n",
    "\n",
    "\n",
    "## What is a Nuisance Parameter?\n",
    "\n",
    "***A nuisance parameter is any quantity whose value is not relevant to the goal of an analysis, but is nevertheless required to determine some quantity of interest***.\n",
    "\n",
    "For example, last week, in problem 3, we estimated both the mean $C$ and shape $\\sigma_\\text{source}$ for some spherical cow galaxy. Now the shape might be important, but in homework 1, you only needed the brightness of the sources where you plotted up $r$ vs $g-i$ for galaxies (you needed the shape to separate stars from galaxies in the form of MEAN_OBJECT_TYPE, but that's it).\n",
    "\n",
    "As far as you were concerned in homework 1, the shape was a **nuisance** parameter.\n",
    "\n",
    "\n",
    "## A Classic Problem:\n",
    "\n",
    "\n",
    "We'll start with an example of nuisance parameters that, in one form or another, dates all the way back to the posthumous [1763 paper](http://www.stat.ucla.edu/history/essay.pdf) written by Thomas Bayes himself. The particular version of this problem we'll study is borrowed from [Eddy 2004](ftp://selab.janelia.org/pub/publications/Eddy-ATG3/Eddy-ATG3-reprint.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The setting is a rather contrived game in which Alice and Bob bet on the outcome of a process they can't directly observe:\n",
    "\n",
    "> Alice and Bob enter a room. Behind a curtain there is a billiard table, which they cannot see, but their friend Carol can. Carol rolls a ball down the table, and marks where it lands. Once this mark is in place, Carol begins rolling new balls down the table. If the ball lands to the left of the mark, Alice gets a point; if it lands to the right of the mark, Bob gets a point.  We can assume for the sake of example that Carol's rolls are unbiased: that is, the balls have an equal chance of ending up anywhere on the table.  **The first person to reach six points wins the game.**\n",
    "\n",
    "Here ***the location of the mark (determined by the first roll) can be considered a nuisance parameter***.\n",
    "\n",
    "It is unknown, and not of immediate interest, but it clearly must be accounted for when predicting the outcome of subsequent rolls. If the first roll settles far to the right, then subsequent rolls will favor Alice. If it settles far to the left, Bob will be favored instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this setup, here is the question we ask of ourselves:\n",
    "\n",
    "> In a particular game, after eight rolls, Alice has five points and Bob has three points. What is the probability that Bob will go on to win the game?\n",
    "\n",
    "Intuitively, you probably realize that because Alice received five of the eight points, the marker placement likely favors her. And given this, it's more likely that the next roll will go her way as well. \n",
    "\n",
    "And she has three opportunities to get a favorable roll before Bob can win; she seems to have clinched it.  But, **quantitatively**, what is the probability that Bob will squeak-out a win?\n",
    "\n",
    "\n",
    "Someone following a classical frequentist approach might reason as follows:\n",
    "\n",
    "To determine the result, we need an intermediate estimate of where the marker sits. We'll quantify this marker placement as a probability $p$ that any given roll lands in Alice's favor.  Because five balls out of eight fell on Alice's side of the marker, we can quickly show that the maximum likelihood estimate of $p$ is given by:\n",
    "\n",
    "$$\n",
    "\\hat{p} = 5/8\n",
    "$$\n",
    "\n",
    "(This result follows in a straightforward manner from the [binomial likelihood](http://en.wikipedia.org/wiki/Binomial_distribution)). \n",
    "\n",
    "# 1.1 Under the assumptions of maximum likelihood, what is the probability of Bob winning the game - i.e. getting six points, and what are the odds \n",
    "# (i.e. $(1 - p_\\text{Bob wins})/p_\\text{Bob wins}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To win the game, Bob has to score three more points consecutively. Given $p = \\frac{5}{8}$ from MLE, his winning probability is $(1-p)^{3} = (\\frac{3}{8})^{3} = \\frac{27}{512} = 5.273\\%.$ The odds are 485 to 27 (or 17.963 to 1) against."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also approach this problem from a Bayesian standpoint. This is slightly more involved, and requires us to first define some notation.\n",
    "\n",
    "We'll consider the following random variables:\n",
    "\n",
    "- $B$ = Bob Wins\n",
    "- $D$ = observed data, i.e. $D = (n_A, n_B) = (5, 3)$\n",
    "- $p$ = unknown probability that a ball lands on Alice's side during the current game\n",
    "\n",
    "We want to compute $P(B~|~D)$; that is, the probability that Bob wins given our observation that Alice currently has five points to Bob's three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general Bayesian method of treating nuisance parameters is ***marginalization***, or integrating the joint probability over the entire range of the nuisance parameter. In this case, that means that we will first calculate the joint distribution\n",
    "$$\n",
    "P(B,p~|~D)\n",
    "$$\n",
    "and then marginalize over $p$ using the following identity:\n",
    "$$\n",
    "P(B~|~D) \\equiv \\int_{-\\infty}^\\infty P(B,p~|~D) {\\mathrm d}p\n",
    "$$\n",
    "This identity follows from the definition of conditional probability, and the law of total probability: that is, it is a fundamental consequence of probability axioms and will always be true. Even a frequentist would recognize this; they would simply disagree with our interpretation of $P(p)$ as being a measure of uncertainty of our own knowledge.\n",
    "\n",
    "To compute this result, we will manipulate the above expression for $P(B~|~D)$ until we can express it in terms of other quantities that we can compute.\n",
    "\n",
    "We'll start by applying the following definition of [conditional probability](http://en.wikipedia.org/wiki/Conditional_probability#Definition) to expand the term $P(B,p~|~D)$:\n",
    "\n",
    "$$\n",
    "P(B~|~D) = \\int P(B~|~p, D) P(p~|~D) dp\n",
    "$$\n",
    "\n",
    "Next we use [Bayes' rule](http://en.wikipedia.org/wiki/Bayes%27_theorem) to rewrite $P(p~|~D)$:\n",
    "\n",
    "$$\n",
    "P(B~|~D) = \\int P(B~|~p, D) \\frac{P(D~|~p)P(p)}{P(D)} dp\n",
    "$$\n",
    "\n",
    "Finally, using the same probability identity we started with, we can expand $P(D)$ in the denominator to find:\n",
    "\n",
    "$$\n",
    "P(B~|~D) = \\frac{\\int P(B~|~p,D) P(D~|~p) P(p) dp}{\\int P(D~|~p)P(p) dp}\n",
    "$$\n",
    "\n",
    "Now the desired probability is expressed in terms of three quantities that we can compute. Let's look at each of these in turn:\n",
    "<small>\n",
    "- $P(B~|~p,D)$: This term is exactly the frequentist likelihood we used above. In words: given a marker placement $p$ and the fact that Alice has won 5 times and Bob 3 times, what is the probability that Bob will go on to six wins?  \n",
    "    \n",
    "    \n",
    "- $P(D~|~p)$: this is another easy-to-compute term. In words: given a probability $p$, what is the likelihood of exactly 5 positive outcomes out of eight trials? The answer comes from the well-known [Binomial distribution](http://en.wikipedia.org/wiki/Binomial_distribution)\n",
    "    \n",
    "    \n",
    "- $P(p)$: this is our prior on the probability $p$. By the problem definition, we can assume that $p$ is evenly drawn between 0 and 1.  That is, $P(p) \\propto 1$, and the integrals range from 0 to 1.</small>\n",
    "\n",
    "# 1.2 Evaluate that integral and get an expression in terms of $p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(B~|~p,D) = (1-p)^{3}$\n",
    "\n",
    "$P(D~|~p) = \\frac{8!}{5!3!}p^{5}(1-p)^{3} = 56p^{5}(1-p)^{3}$\n",
    "\n",
    "$P(p) = 1$\n",
    "\n",
    "Therefore, $$\n",
    "P(B~|~D) = \\frac{\\int_0^1 P(B~|~p,D) P(D~|~p) P(p) dp}{\\int_0^1 P(D~|~p)P(p) dp} = \\frac{\\int_0^1 p^{5}(1-p)^{6} dp}{\\int_0^1 p^{5}(1-p)^{3} dp} = \\frac{\\beta(7, 6)}{\\beta(4, 6)} = \\frac{\\frac{6! 5!} {12 !}}{\\frac{3! 5!}{9!}} = \\frac{1}{11}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integrals you find might look a bit difficult, but they are just special cases of the [Beta Function](http://en.wikipedia.org/wiki/Beta_function):\n",
    "$$\n",
    "\\beta(n, m) = \\int_0^1 (1 - p)^{n - 1} p^{m - 1}\n",
    "$$\n",
    "\n",
    "scipy has an implementation of this in `scipy.special`.\n",
    "\n",
    "# 1.3 Evaluate your expression numerically and compute the probability and the odds that Bob wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability is 0.0909090909090909, and the odds are 10.000 to 1 against.\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import beta\n",
    "prob = beta(7,6)/beta(4,6)\n",
    "odds = (1-prob)/prob\n",
    "print(f'The probability is {prob:.16f}, and the odds are {odds:.3f} to 1 against.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Finally, lets check the result using an approach called Monte Carlo, where we simulate a bunch of games, and simply count the fraction of relevant games that Bob goes on to win. The current problem is especially simple because so many of the random variables involved are uniformly distributed. \n",
    "\n",
    "* Simulate 100,000 random p between 0 and 1 - this will be a 1D array\n",
    "* given each p, to win the game needs *at most* 11 rolls - simulate 11 rolls for each p - this will be a 2D array\n",
    "* count the cumultative wins for Alice and Bob at each roll - this is a 2D array\n",
    "* determine which games has Bob with three points by the end of game 8 - this is a 1D array\n",
    "* considering only these games, find the number of games which Bob won (i.e. has six points at the end of game 11)\n",
    "* compute the total probability that Bob won, and the odds that Bob won.\n",
    "\n",
    "\n",
    "## You don't need anything more than `numpy` to do this - in particular `numpy.random` and `numpy.cumsum` to do this.\n",
    "No for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def MC(P,size): \n",
    "    games = 11\n",
    "    Rolls = np.random.rand(size,games)\n",
    "    ACumWin = np.cumsum(Rolls < np.repeat(P,games).reshape(size,games),axis=1)\n",
    "    BCumWin = np.repeat((np.arange(games)+1,),size,axis=0) - ACumWin\n",
    "    BGet3AtG8 = (BCumWin[:,7] == 3)\n",
    "    BWinAtG11 = BGet3AtG8&(BCumWin[:,10] == 6)\n",
    "\n",
    "    total = BGet3AtG8.sum()\n",
    "    Bwin = BWinAtG11.sum()\n",
    "    prob = Bwin/total\n",
    "    odds = (total-Bwin,Bwin)\n",
    "    print(f'The total probability that Bob won is {prob:.5f}\\nThe odds that Bob Won are {odds[0]} to {odds[1]} (or {odds[0]/odds[1]:.3f} to 1) against.')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total probability that Bob won is 0.09062\n",
      "The odds that Bob Won are 10055 to 1002 (or 10.035 to 1) against.\n"
     ]
    }
   ],
   "source": [
    "size = 100000\n",
    "\n",
    "# p is is evenly drawn between 0 and 1 (uniform prior)\n",
    "p = np.random.rand(size)\n",
    "MC(p,size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result agrees what we got in 1.2 above. In fact, the Monte Carlo method provides us a \"numerical experiment\" to evaluate the Bayesian posterior probability $P(B~|~D) = \\frac{\\int_0^1 P(B~|~p,D) P(D~|~p) P(p) dp}{\\int_0^1 P(D~|~p)P(p) dp}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Why do the two results disagree?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of frequentist and Bayesian approaches disagree. The difference mainly comes from what prior $P(p)$ to use in each statistical inference. The frequentist prior is essentially a delta function $P(p) = \\delta(p=\\frac{5}{8})$ given by the maximum likelihood estimate for $P(D~|~p)$, and thus the frequentist approach only considers the single case $p=\\frac{5}{8}$. However, the Bayesian prior is a uniform prior $P(p) = 1$ drawn between 0 and 1, which indeed takes into account, for example, cases with smaller $p$ values but larger $P(B~|~p,D)P(D~|~p)$ products. As what we found above, the overall inclusion of all possible $p$ in Bayesian approach results in a higher winning probability for Bob.\n",
    "\n",
    "To check my argument, we can replace the random prior $p$ used in 1.4 with a fixed prior $p=\\frac{5}{8}$. We should recover the probability and odds we got in 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total probability that Bob won is 0.05242\n",
      "The odds that Bob Won are 26807 to 1483 (or 18.076 to 1) against.\n"
     ]
    }
   ],
   "source": [
    "#Fixed p = 5/8 from MLE:\n",
    "p = np.repeat(5/8,size)\n",
    "MC(p,size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the Monte Carlo simulation with fixed prior agrees the frequentist results."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:fds] *",
   "language": "python",
   "name": "conda-env-fds-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
