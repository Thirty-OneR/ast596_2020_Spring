{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1920,\n",
    "        'height': 1080,\n",
    "        'scroll': True,\n",
    "})\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 05, ASTR 596: Fundamentals of Data Science\n",
    "\n",
    "\n",
    "## Sampling and MCMC\n",
    "\n",
    "### Gautham Narayan \n",
    "##### <gsn@illinois.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center> HW3 solutions are up. </center>\n",
    "\n",
    "### <center> No one had conceptual statistics issues with HW3, but if you got an answer to Q3 that for whatever reason didn't match Q2, you probably want to look at array indexing in python/numpy </center>\n",
    "\n",
    "https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap: Optimization/Sampling?\n",
    "\n",
    "* We've come up with a model, an objective/loss/likelihood function and some priors\n",
    "    * Now we actually want to evaluate the posterior $P(\\theta|D)$\n",
    "    * Inference on grids is too ineffective and limiting\n",
    "* Local optimizers\n",
    "    * typically use computed gradient information to move in direction of increase in likelihood\n",
    "        * great at finding local minima for you to get stuck in\n",
    "        * usually good for low-dimensional problems that are ideally linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Global optimizers\n",
    "    * typically combine some wide search with local refinement \n",
    "        * usually have some tuning parameters \n",
    "            * temperature scale with simulated annealing, step size with basin hopping   \n",
    "\n",
    "* **Curse of dimensionality** - as the number of parameters of your problem grows, it becomes increasingly ineffective to search the full volume for the tiny region where the posterior is significantly non-zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Alternate approach: Monte Carlo\n",
    "    * Draw repeated random samples of your parameters\n",
    "    * Keep the samples that match your criterion \n",
    "        * Approximates distribution of arbitrary functions of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Conditional probability](conditional_prob2.png)\n",
    "Courtesy: Federica Bianco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Nice because:\n",
    "    * convert messy complicated integrals of high-dimensional non-linear functions to simple **numerical sums!!!**.\n",
    "    * easy to marginalize out nuisance parameters (literally, just ignore that parameter - i.e. marginalize it)\n",
    "    * want credible regions for a parameter of interest? Get from distribution of samples directly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Conditional probability](conditional_prob3.png)\n",
    "Courtesy: Federica Bianco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Simple MC with uniform sampling of parameter space **does not solve curse of dimensionality (too many useless samples in low likelihood region)** \n",
    "* What if, instead of sampling the parameter space uniformly, you could sample the posterior directly\n",
    "    * Possible outcomes would be **simulated with a frequency proportional to the probability**\n",
    "    \n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"Likelihood_Surface.png\" width=100%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There are a couple of approaches to this:\n",
    " \n",
    "### 1. Rejection sampling\n",
    "For this method, we need to define an *envelope function* which everywhere exceeds the target PDF, $p(x)$, and can be sampled. Let this be $Ag(x)$ where $A$ is a scaling factor and $g(x)$ is a PDF we know.\n",
    "\n",
    "Then the algorithm is\n",
    "```\n",
    "while we want more samples\n",
    "    draw a random value for x from some distribution g in the variable x\n",
    "    draw u from Uniform(0,1)\n",
    "    if u <= p(x)/(A*g(x)), keep the sample x\n",
    "    otherwise, reject x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Visually, this corresponds to drawing points that uniformly fill in the space under $p(x)$.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"mc1_rejection.png\" width=100%></td>\n",
    "    </tr>\n",
    "</table>\n",
    "Courtesy: Phil Marshall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# This also does not solve the problem posed by the curse of dimensionality \n",
    "\n",
    "# To see why work on HW3 Q1\n",
    "\n",
    "# But this approach is general and works for any function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The second approach you've already seen and used a lot:\n",
    "\n",
    "### 2. The Inverse Transform \n",
    "\n",
    "The definition of the CDF (and it's inverse, $F^{-1}$, the quantile function - i.e. `ppf()` in `scipy.stats`) \n",
    "\n",
    "$F(x) = P(X \\leq x) = \\int_{-\\infty}^x p(x')\\,dx'$\n",
    "\n",
    "By this definition, quantiles of $X$ are uniformly distributed on [0,1]. If $F^{-1}$ is easy to evaluate, we can use this straightforwardly:\n",
    "\n",
    "```\n",
    "draw u from Uniform(0,1)\n",
    "compute x = F_inverse(u)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>PDF<img src=\"mc1_invtrans0.png\" width=100%></td>\n",
    "        <td></td>\n",
    "        <td>CDF<img src=\"mc1_invtrans1.png\" width=100%></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Courtesy: Phil Marshall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# In class exercise: The PIT with exponential distributions:\n",
    "\n",
    "This distribution has $p(x)=\\lambda e^{-\\lambda x}$ and $F(x)=1-e^{-\\lambda x}$ for $x\\geq0$.\n",
    "\n",
    "The quantile function is, therefore, $F^{-1}(P) = -\\ln(1-P)/\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Here's some code for the inverse tranform\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def inv_trans_demo(x, lam):\n",
    "    hist = plt.hist(x, bins=50, normed=True)\n",
    "    xs = np.linspace(0.0, 10.0/lam, 100)\n",
    "    pdf = lam * np.exp(-lam*xs)\n",
    "    pdfline = plt.plot(xs, pdf, 'r', lw=2)\n",
    "    plt.xlabel(r'x', fontsize=22)\n",
    "    plt.ylabel(r'P(x)', fontsize=22);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# for lam = 1., draw a 1000 random samples from a unform distribution\n",
    "# and use inverse CDF to convert to samples from a exponential distribution\n",
    "# then use the demo code to histogram your samples and overplot the distribution.\n",
    "lam = 1.0\n",
    "u = np.random.rand(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "* This does solve the problem posed by the curse of dimensionality \n",
    "\n",
    "* No sample is rejected! \n",
    "\n",
    "* The con is that you have to know what p(x) looks like in advance\n",
    "\n",
    "* If p(x) is your posterior, then you not only need to be able to solve for it analytically (including the evidence - the denominator of Bayes' theorem) but then you've got to figure out how to invert it... even harder.\n",
    "    \n",
    "* There is a place for the PIT, but we started down this road because our functions weren't generally going to be nice, so lets deal with rejection sampling some more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# We want a couple of properties\n",
    "\n",
    "* We want to sample the full distribution \n",
    "* We want the frequency of samples between $x$ and $x+dx$ to be proportional to $p(x)dx$\n",
    "\n",
    "What if, instead of drawing i.i.d samples, we drew samples such that they are correlated with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You have already seen examples of processes where samples are correlated with each other - Brownian motion/random walks/Wiener processes - all of these are examples of **stochastic** processes\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"Wiener_process_3d.png\" width=100%></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " What if we chose our samples to be correlated in a very specific way:\n",
    "   * decide how many samples you want\n",
    "   * start somewhere - we'll call our current position in k-dimensional parameter space $x$\n",
    "   * while you want samples:\n",
    "       * perturb $x$ to $x'$ by some random vector drawn from a fixed distribution   \n",
    "           * i.e. the samples are correlated\n",
    "       * evaluate your function at $x'$ and $x$\n",
    "       * if the function is higher at $x'$ than at $x$ \n",
    "           * then yay! Accept it, and set the position $x'$ to the current position $x$\n",
    "       * else if the function is lower at $x'$ than at $x$\n",
    "           * well maybe that's bad, or maybe we're just unlucky and there's good samples to be had near here\n",
    "           * How do we decide? Well let's draw a random number and check if our function ratio is better or worse\n",
    "               * if it's better, accept and set the position $x'$ to the current position $x$\n",
    "               * else reject and update the current position to be the same \n",
    "       * stick the current position after you did this into a list of samples \n",
    "    \n",
    "This sequence/list of all accepted samples is a **chain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is just our **rejection sampling** strategy (recall):\n",
    "\n",
    "```\n",
    "while we want more samples\n",
    "    draw a random value for x from some distribution g in the variable x\n",
    "    draw u from Uniform(0,1)\n",
    "    if u <= p(x)/(A*g(x)), keep the sample x\n",
    "    otherwise, reject x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Markov Chains:\n",
    "\n",
    "If we can construct a sequence of samples/chain this way, it will be **ergodic** - i.e. given enough time, the full distribution will be sampled\n",
    "\n",
    "This chain is from some n-dimensional parameter space, with a distribution that is asymptotically proportional to $p(x)$. \n",
    "\n",
    "**(NOTE THAT I SHOULD REALLY BE WRITING $\\theta$ NOT $x$ BUT YOU TRYING DOING THIS OVER AND OVER!)**\n",
    "\n",
    "The constant of proportionality is not important in the first class of problems we will look at. \n",
    "\n",
    "In model comparison problems, the proportionality constant must be known. We've glossed over that so far, so we will blithely push forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "With our particular strategy, every n+1 th position on the chain depends **only** on the nth position:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"MarkovChain.png\" width=100%></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Chains that have this property are called **Markov Chains**.\n",
    "\n",
    "The **state space** of this stochastic process is the set of all possible values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Go here:\n",
    "[http://setosa.io/ev/markov-chains/](http://setosa.io/ev/markov-chains/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* If you have a finite state space, then you write down transition probabilities from one state to another as a matrix or represent it with a graph\n",
    "\n",
    "* Usually in science, our parameters do not have discrete states, but take values from the set of real numbers,  ${\\rm I\\!R}$ - i.e. there are an (uncountably) infinite number of states, and we don't have single dimensional problems\n",
    "\n",
    "    * Can't represent this as a matrix/graph anymore\n",
    "    \n",
    "* Essential elements of this theory still hold in an infinite dimensional state space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As long as the Markov chain is **positive recurrent** (i.e. you can get to any parameter in a finite number of steps) and is **irreducible** (you can get to every parameter value from every other parameter value) then it has another nice property  - it is **stationary** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A subset of Markov Chains are **stationary**\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"StationaryMarkovChain.png\" width=100%></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "The average of some function over the samples in the Markov chain is asymptotically equal to the expectation value of the function over the underlying stationary distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"ReversibleMarkovChain.png\" width=100%></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "### **All reversible chains are stationary, but not vice-versa!** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The particular algorithm for generating new samples (more properly detailed in HW4) is called **Metropolis-Hastings** after the folks that came up with it.\n",
    "\n",
    "In summary, the Metropolis-Hastings algorithm consists of these steps:\n",
    "\n",
    "1. given $x$ and $T(x'|x)$, draw a proposed value for $x'$\n",
    "\n",
    "2. compute acceptance probability $p_{\\rm acc}(x,x')$.\n",
    "\n",
    "3. draw a random number between 0 and 1 from a uniform distribution; if it smaller than $p_{\\rm acc}(x,x')$, then accept $x'$.\n",
    "\n",
    "4. if $x'$ is accepted added it to the chain, if not, add $x'$ to the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thirty-oner/anaconda3/lib/python3.7/site-packages/IPython/core/display.py:701: UserWarning: Consider using IPython.display.IFrame instead\n",
      "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Vv3f0QNWvWQ?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Vv3f0QNWvWQ?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This process is NOT **stationary**. \n",
    "\n",
    "#### Why does it work?\n",
    "The probability of an arbitrary point from such a chain being located at $x'$ is (marginalizing over the possible immediately preceding points)\n",
    "\n",
    "## $$p(x') = \\int dx \\, p(x) \\, T(x'|x)$$\n",
    "\n",
    "where $T(x'|x)$ is the transition probability of a step from $x$ to $x'$.\n",
    "\n",
    "If we have detailed balance, \n",
    "\n",
    "## $$p(x)T(x'|x) = p(x')T(x|x')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "rearranging:\n",
    "\n",
    "## $$ \\frac{T(x'|x)}{T(x|x')} = \\frac{p(x')}{p(x)} $$\n",
    "\n",
    "The basic trick to connect this with rejection sampling is to break the transition into two steps:\n",
    "1. A proposal, g(x'| x)\n",
    "and \n",
    "2. Acceptance ratio, A(x'|x)\n",
    "\n",
    "i.e. \n",
    "\n",
    "## $$ T(x'|x) = A(x'|x) g(x'| x) $$ \n",
    "\n",
    "rearranging again :\n",
    "\n",
    "## $$ \\frac{A(x'|x)}{A(x|x')} = \\frac{p(x')g(x|x')}{p(x)g(x'|x) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice that the probability of accepting a step  (once it's proposed) is\n",
    "\n",
    "## $$A(x',x) = \\mathrm{min}\\left[1, \\frac{p(x')g(x|x')}{p(x)g(x'|x)}\\right]$$\n",
    "\n",
    "Let's look again at the requirement of detailed balance\n",
    "\n",
    "> the probability of being at $x$ and moving to $y$ must equal the probability of being at $y$ and moving to $x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The first of these is $p(x)g(x'|x)A(x',x)$, where\n",
    "\n",
    "* $p(x)$ is the posterior density (probability of *being* at $x$, if we're sampling $P$ properly)\n",
    "\n",
    "* $g(x'|x)$ is the proposal distribution (probability of attempting a move to $x'$ from $x$)\n",
    "\n",
    "* $A(x',x)$ is the probability of accepting the proposed move\n",
    "\n",
    "With this definition of $A$, detailed balance is automatically satisfied!\n",
    "\n",
    "## $$p(x)g(x'|x)A(x',x) \\equiv p(x')g(x|x')A(x,x')$$\n",
    "\n",
    "Note that **even if a step is rejected, we still keep a sample** (the original state, without moving). The difficulty of finding a temptingly better point is important information!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How far should we step (small steps in parameter space or large). This impacts the efficiency of the process but not if we will reach equilibrium.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"sampling.png\" width=100%></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>\"Well that's easy, MCMC generates samples from the posterior distribution by constructing an ergodic, reversible Markov-chain that has as its equilibrium distribution the target posterior distribution. Questions?\" </center>\n",
    "### <center> - Thomas Wiecki, very tongue in cheek </center>\n",
    "\n",
    "https://twiecki.io/blog/2015/11/10/mcmc-sampling/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "* burn in/stationary\n",
    "* affine-invariant MC\n",
    "* gibbs sampling\n",
    "    * conjugate priors -> PIT\n",
    "* PT -> simmulated annealing \n",
    "\n",
    "* using MCMC\n",
    "    * regularization\n",
    "        * bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
